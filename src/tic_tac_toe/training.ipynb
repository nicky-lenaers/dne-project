{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gymnasium.spaces import Discrete, Space\n",
    "from tianshou.env import PettingZooEnv\n",
    "from tianshou.policy import BasePolicy, DQNPolicy\n",
    "from tianshou.utils.net.common import Net\n",
    "from torch import nn\n",
    "\n",
    "def create_dqn_agent(env: PettingZooEnv) -> BasePolicy:\n",
    "    \"\"\"\n",
    "    Create an agent by defining a policy which determines how the\n",
    "    agent should behave.\n",
    "    \"\"\"\n",
    "    model = create_model(env=env)\n",
    "\n",
    "    return DQNPolicy(\n",
    "        model=model,\n",
    "        optim=torch.optim.Adam(model.parameters(), lr=1e-4),\n",
    "        discount_factor=0.9,\n",
    "        estimation_step=3,\n",
    "        target_update_freq=320,\n",
    "    )\n",
    "\n",
    "def create_model(env: PettingZooEnv) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Create the deep learning model that underpins the behaviour\n",
    "    of the agent (it is not the agent itself).\n",
    "    \"\"\"\n",
    "    state_space: Space = env.observation_space[\"observation\"]\n",
    "    action_space: Discrete = env.action_space\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    return Net(\n",
    "        state_shape=state_space.shape,\n",
    "        action_shape=action_space.n,\n",
    "        hidden_sizes=[128, 128, 128, 128],\n",
    "        device=device,\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LLM Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "from tianshou.data import Batch\n",
    "from tianshou.policy import BasePolicy\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "class LLMAgent(BasePolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            pretrained_model_name_or_path=\"microsoft/phi-2\",\n",
    "            torch_dtype=torch.float32,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(\n",
    "            pretrained_model_name_or_path=\"microsoft/phi-2\",\n",
    "            torch_dtype=torch.float32,\n",
    "            # device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, batch: Batch, state: dict | Batch | np.ndarray | None = None\n",
    "    ) -> Batch:\n",
    "        print(\"obs: \", batch[\"obs\"][\"obs\"])\n",
    "        current_obs = batch[\"obs\"][\"obs\"]\n",
    "        prompt = \"What is the capital of France?\"\n",
    "        token_ids = self.tokenizer.encode(\n",
    "            prompt, add_special_tokens=False, return_tensors=\"pt\"\n",
    "        )\n",
    "        output_ids = self.llm.generate(\n",
    "            token_ids.to(self.llm.device),\n",
    "            max_new_tokens=20,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        output = self.tokenizer.decode(output_ids[0][token_ids.size(1) :])\n",
    "        print(\"LLM OUTPUT: \", output)\n",
    "        return Batch(act=[1])\n",
    "\n",
    "    def learn(self, batch: Batch) -> Dict[str, Any]:\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.policy import MultiAgentPolicyManager, RandomPolicy\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "\n",
    "env = PettingZooEnv(tictactoe_v3.env())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = LLMAgent() # or create_dqn_agent(env)\n",
    "\n",
    "policies = [agent, RandomPolicy()]\n",
    "policy = MultiAgentPolicyManager(policies=policies, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.env import DummyVectorEnv, PettingZooEnv\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from tianshou.data import Collector\n",
    "\n",
    "env = DummyVectorEnv([lambda: PettingZooEnv(tictactoe_v3.env())])\n",
    "collector = Collector(policy=policy, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = collector.collect(n_episode=1, render=.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
