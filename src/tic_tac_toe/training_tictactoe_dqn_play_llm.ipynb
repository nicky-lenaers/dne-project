{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training DQN for Tic Tac Toe and play against LLM\n",
    "\n",
    "Based on [RL against random policy opponent with PettingZoo](https://tianshou.org/en/stable/01_tutorials/04_tictactoe.html)."
   ],
   "metadata": {
    "id": "MWc4kcBMGnpg"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_ixzdtHHYg_a"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Install dependencies"
   ],
   "metadata": {
    "id": "y9lmQIv9F9Ke"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install gymnasium==0.29.1 pygame==2.3.0 pettingzoo==1.24.3 tianshou==0.5.1 transformers==4.39.1 accelerate==0.28.0"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QSgELpG0YrrI",
    "outputId": "2e14d4af-ecc1-42e1-b957-06bc0ecb7502"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting gymnasium==0.29.1\n",
      "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m953.9/953.9 kB\u001B[0m \u001B[31m6.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pygame==2.3.0\n",
      "  Downloading pygame-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.8/13.8 MB\u001B[0m \u001B[31m6.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pettingzoo==1.24.3\n",
      "  Downloading pettingzoo-1.24.3-py3-none-any.whl (847 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m847.8/847.8 kB\u001B[0m \u001B[31m21.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting tianshou==0.5.1\n",
      "  Downloading tianshou-0.5.1-py3-none-any.whl (163 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m163.1/163.1 kB\u001B[0m \u001B[31m13.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting transformers==4.39.1\n",
      "  Downloading transformers-4.39.1-py3-none-any.whl (8.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m8.8/8.8 MB\u001B[0m \u001B[31m39.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting accelerate==0.28.0\n",
      "  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m290.1/290.1 kB\u001B[0m \u001B[31m10.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1) (1.25.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1) (4.11.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium==0.29.1)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tianshou==0.5.1) (4.66.2)\n",
      "Requirement already satisfied: tensorboard>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from tianshou==0.5.1) (2.15.2)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from tianshou==0.5.1) (2.2.1+cu121)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from tianshou==0.5.1) (0.58.1)\n",
      "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from tianshou==0.5.1) (3.9.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tianshou==0.5.1) (24.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.1) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.1) (0.20.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.1) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.1) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.1) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.1)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.6/3.6 MB\u001B[0m \u001B[31m35.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.1) (0.4.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0) (5.9.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.1) (2023.6.0)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->tianshou==0.5.1) (0.41.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.1) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.1) (1.62.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.1) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.1) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.1) (3.6)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.1) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.1) (67.7.2)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.1) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.1) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.5.0->tianshou==0.5.1) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.1) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->tianshou==0.5.1) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->tianshou==0.5.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->tianshou==0.5.1) (3.1.3)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.4.0->tianshou==0.5.1)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.4.0->tianshou==0.5.1)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.4.0->tianshou==0.5.1)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.4.0->tianshou==0.5.1)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.4.0->tianshou==0.5.1)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.4.0->tianshou==0.5.1)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.4.0->tianshou==0.5.1)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.4.0->tianshou==0.5.1)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.4.0->tianshou==0.5.1)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.4.0->tianshou==0.5.1)\n",
      "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.4.0->tianshou==0.5.1)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->tianshou==0.5.1) (2.2.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->tianshou==0.5.1)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.5.0->tianshou==0.5.1) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.5.0->tianshou==0.5.1) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.5.0->tianshou==0.5.1) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.5.0->tianshou==0.5.1) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.5.0->tianshou==0.5.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->tianshou==0.5.1) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.5.0->tianshou==0.5.1) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.5.0->tianshou==0.5.1) (3.2.2)\n",
      "Installing collected packages: farama-notifications, pygame, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gymnasium, pettingzoo, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, tianshou, accelerate\n",
      "  Attempting uninstall: pygame\n",
      "    Found existing installation: pygame 2.5.2\n",
      "    Uninstalling pygame-2.5.2:\n",
      "      Successfully uninstalled pygame-2.5.2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.40.0\n",
      "    Uninstalling transformers-4.40.0:\n",
      "      Successfully uninstalled transformers-4.40.0\n",
      "Successfully installed accelerate-0.28.0 farama-notifications-0.0.4 gymnasium-0.29.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pettingzoo-1.24.3 pygame-2.3.0 tianshou-0.5.1 tokenizers-0.15.2 transformers-4.39.1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import needed dependencies"
   ],
   "metadata": {
    "id": "InU12D5lGB5u"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import Any, Dict\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Space\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import re\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tianshou.data import Batch, Collector, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.policy import (\n",
    "    BasePolicy,\n",
    "    DQNPolicy,\n",
    "    MultiAgentPolicyManager,\n",
    "    RandomPolicy\n",
    ")\n",
    "from tianshou.trainer import OffpolicyTrainer\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from tianshou.utils.net.common import Net\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ],
   "metadata": {
    "id": "jAnd4bSUYuJ2"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup environment"
   ],
   "metadata": {
    "id": "GJUqMfTpGHkb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_env(render_mode=None):\n",
    "  return PettingZooEnv(tictactoe_v3.env(render_mode=render_mode))\n",
    "\n",
    "# create the environment and get the shape of the states and shape of the actions\n",
    "env = get_env()\n",
    "observation_space = env.observation_space['observation'] if isinstance(\n",
    "  env.observation_space, gym.spaces.Dict\n",
    ") else env.observation_space\n",
    "state_shape = observation_space.shape or observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n"
   ],
   "metadata": {
    "id": "LP3tEh8Acfca"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup policies for training DQNPolicy\n",
    "\n",
    "One training policy (DQNPolicy) and the opponent (RandomPolicy)."
   ],
   "metadata": {
    "id": "lB6dVGoGGPl0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Hidden sizes: shape of the MLP\n",
    "hidden_sizes = [128, 128, 128, 128]\n",
    "# device to train on\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# the number of steps to look ahead\n",
    "estimation_step = 3\n",
    "# the target network update frequency (0 if you do not use the target network).\n",
    "target_update_freq = 320\n",
    "# learning rate of the Adam optimizer\n",
    "lr = 1e-4"
   ],
   "metadata": {
    "id": "QLfX7sfzzXQ2"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_dqn_policy():\n",
    "  # The deep learning model (MLP) that underpins the behaviour of the agent (it is not the agent itself).\n",
    "  net = Net(\n",
    "    state_shape,\n",
    "    action_shape,\n",
    "    hidden_sizes=hidden_sizes,\n",
    "    device=device\n",
    "  ).to(device)\n",
    "\n",
    "  # Adam optimizer\n",
    "  optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "  # Agent to learn\n",
    "  return DQNPolicy(\n",
    "    model=net,\n",
    "    optim=optim,\n",
    "    action_space=env.action_space,\n",
    "    estimation_step=estimation_step,\n",
    "    target_update_freq=target_update_freq\n",
    "  )"
   ],
   "metadata": {
    "id": "RP9APMhYzSeg"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the agent with DQN\n",
    "Using the OffPolicyTrainer."
   ],
   "metadata": {
    "id": "wgZy7_UPJzbJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# path to save the results and the logging\n",
    "path = '/content/tic_tac_toe/dqn'\n",
    "\n",
    "def train_policy(policy, agent_id):\n",
    "  # number of training environments\n",
    "  training_num = 100\n",
    "  # number of testing environments\n",
    "  test_num = 100\n",
    "  # size of the VectorReplayBuffer\n",
    "  buffer_size = 20000\n",
    "  # the batch size of sample data, which is going to feed in the policy network.\n",
    "  batch_size = 64\n",
    "  # the maximum number of epochs for training. The training process might be finished before reaching max_epoch if stop_fn is set.\n",
    "  epoch = 100\n",
    "  # the number of transitions collected per epoch.\n",
    "  step_per_epoch = 1000\n",
    "  # the number of transitions the collector would collect before the network update,\n",
    "  # i.e., trainer will collect \"step_per_collect\" transitions and do some policy network update\n",
    "  # repeatedly in each epoch.\n",
    "  step_per_collect = 10\n",
    "  # used in the stop function when the mean rewards are over this threshold\n",
    "  win_rate = 0.9\n",
    "  # The eps for epsilon-greedy exploration for test and training\n",
    "  eps_test = 0.05\n",
    "  eps_train = 0.1\n",
    "  # the number of times the policy network would be updated per transition after (step_per_collect)\n",
    "  # transitions are collected, e.g., if update_per_step set to 0.3, and step_per_collect is 256,\n",
    "  # policy will be updated round(256 * 0.3 = 76.8) = 77 times after 256 transitions are collected\n",
    "  # by the collector. Default to 1.\n",
    "  update_per_step = 0.1\n",
    "\n",
    "\n",
    "  # Dummy vectorized environment wrapper, implemented in for-loop.\n",
    "  # This has the same interface as true vectorized environment, but the rollout does not happen in parallel.\n",
    "  # So, all workers just wait for each other and the environment is as efficient as using a single environment.\n",
    "  # This can be useful for testing or for demonstration purposes.\n",
    "  train_envs = DummyVectorEnv([get_env for _ in range(training_num)])\n",
    "  test_envs = DummyVectorEnv([get_env for _ in range(test_num)])\n",
    "\n",
    "  # VectorReplayBuffer contains n ReplayBuffer with the same size.\n",
    "  # It is used for storing transition from different environments yet keeping the order of time.\n",
    "  vectorReplayBuffer = VectorReplayBuffer(buffer_size, len(train_envs))\n",
    "\n",
    "  # determine whether the action needs to be modified with corresponding policy’s exploration noise.\n",
    "  # If so, “policy. exploration_noise(act, batch)” will be called automatically to add the\n",
    "  # exploration noise into action.\n",
    "  exploration_noise = True\n",
    "\n",
    "  # Train and test collector\n",
    "  # Collector enables the policy to interact with different types of envs with exact number of steps or episodes.\n",
    "  train_collector = Collector(policy, train_envs, vectorReplayBuffer, exploration_noise=exploration_noise)\n",
    "  test_collector = Collector(policy, test_envs, exploration_noise=exploration_noise)\n",
    "\n",
    "  # Collect a specified number of step or episode.\n",
    "  train_collector.collect(n_step=batch_size * training_num)\n",
    "\n",
    "  # A logger that logs statistics during training/testing/updating\n",
    "  writer = SummaryWriter(path)\n",
    "  logger = TensorboardLogger(writer)\n",
    "\n",
    "  # Functions for the OffpolicyTrainer\n",
    "  # Save the best model\n",
    "  def save_best_fn(policy):\n",
    "    torch.save(policy.policies[agent_id].state_dict(), path + '/policy-' + agent_id + '.pth')\n",
    "\n",
    "  # When to stop training\n",
    "  def stop_fn(mean_rewards):\n",
    "    return mean_rewards >= win_rate\n",
    "\n",
    "  # a hook called at the beginning of training in each epoch. It can be used to perform custom additional operations\n",
    "  def train_fn(epoch, env_step):\n",
    "      # Set the eps for epsilon-greedy exploration.\n",
    "      policy.policies[agent_id].set_eps(eps_train)\n",
    "\n",
    "  def test_fn(epoch, env_step):\n",
    "      # Set the eps for epsilon-greedy exploration.\n",
    "      policy.policies[agent_id].set_eps(eps_test)\n",
    "\n",
    "  # A function with signature used in multi-agent RL.\n",
    "  # We need to return a single scalar for each episode’s result to monitor training in the multi-agent RL setting.\n",
    "  # This function specifies what is the desired metric, e.g., the reward of agent 1 or the average reward over all agents.\n",
    "  def reward_metric(rews):\n",
    "    if agent_id == 'player_2':\n",
    "      return rews[:, 1]\n",
    "    return rews[:, 0]\n",
    "\n",
    "  # Offpolicy trainer, samples mini-batches from buffer and passes them to update.\n",
    "  result = OffpolicyTrainer(\n",
    "    policy,\n",
    "    train_collector,\n",
    "    test_collector,\n",
    "    epoch,\n",
    "    step_per_epoch,\n",
    "    step_per_collect,\n",
    "    test_num,\n",
    "    batch_size,\n",
    "    train_fn=train_fn,\n",
    "    test_fn=test_fn,\n",
    "    stop_fn=stop_fn,\n",
    "    save_best_fn=save_best_fn,\n",
    "    update_per_step=update_per_step,\n",
    "    logger=logger,\n",
    "    test_in_train=False, # whether to test in the training phase.\n",
    "    reward_metric=reward_metric\n",
    "  ).run()"
   ],
   "metadata": {
    "id": "Sb9XhtchcB3G"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "agent_learn_player1 = get_dqn_policy()\n",
    "agent_learn_player2 = get_dqn_policy()\n",
    "agent_random = RandomPolicy()\n",
    "\n",
    "agents_1 = [agent_learn_player1, agent_random]\n",
    "agents_2 = [agent_random, agent_learn_player2]\n",
    "\n",
    "# Multi-agent policy manager for Multi-Agent Reinforcement Learning (https://tianshou.org/en/stable/01_tutorials/07_cheatsheet.html#marl-example)\n",
    "policy_1 = MultiAgentPolicyManager(agents_1, env)\n",
    "policy_2 = MultiAgentPolicyManager(agents_2, env)\n",
    "\n",
    "train_policy(policy_1, 'player_1')\n",
    "train_policy(policy_2, 'player_2')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vt7IXWwzc1L0",
    "outputId": "ce1a6951-1db9-4753-bd08-42227a54ab30"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "%tensorboard --logdir /content/tic_tac_toe/dqn"
   ],
   "metadata": {
    "id": "CuAiQM_OTxtg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load best trained agents"
   ],
   "metadata": {
    "id": "6ndBZiU7-UYu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "agent_learn_player1.load_state_dict(torch.load(path + '/policy-player_1.pth'))\n",
    "agent_learn_player2.load_state_dict(torch.load(path + '/policy-player_2.pth'))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Juz2aCgK9mqV",
    "outputId": "9e64428e-db96-4ec5-abd2-bcb8e9b51903"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Play agent against agent function\n"
   ],
   "metadata": {
    "id": "nxLtBMTBKNg-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def play(agent1, agent2, n_episode=100):\n",
    "  env = get_env(render_mode=None)\n",
    "  policy = MultiAgentPolicyManager([agent1, agent2], env)\n",
    "  dummy_vector_env = DummyVectorEnv([lambda: env])\n",
    "  collector = Collector(policy, dummy_vector_env, exploration_noise=True)\n",
    "  # play number of episodes\n",
    "  result = collector.collect(n_episode=n_episode, render=None)\n",
    "  rews, lens = result[\"rews\"], result[\"lens\"]\n",
    "  print(f\"Final reward: {rews[:, 0].mean()}, length: {lens.mean()}\")\n",
    "\n",
    "  won = 0\n",
    "  draw = 0\n",
    "  lost = 0\n",
    "  for res in result['rews']:\n",
    "    if res[0] == 1:\n",
    "      won += 1\n",
    "    elif res[0] == -1:\n",
    "      lost +=1\n",
    "    else:\n",
    "      draw += 1\n",
    "\n",
    "  print(\"Win: \" + str(won) + \" lost: \" + str(lost) + \" draw: \" + str(draw))\n",
    "\n",
    "  return (won, lost, draw)"
   ],
   "metadata": {
    "id": "azNoG8F235Ch"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup LLM Agent"
   ],
   "metadata": {
    "id": "kJt3lAX1qm4X"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class LLMAgent(BasePolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        models_dict = {\"StableLM Zephyr 3B\": \"stabilityai/stablelm-zephyr-3b\"}\n",
    "        model_id = models_dict[\"StableLM Zephyr 3B\"]\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)#, device=0)\n",
    "        self.pipe = pipeline(\"text-generation\", model=model_id, device_map=\"auto\", tokenizer=self.tokenizer, torch_dtype=torch.bfloat16)\n",
    "        self.cache = {}\n",
    "\n",
    "    def field_to_string(self, field, c):\n",
    "      return_value = \"\"\n",
    "      if field[0] == 1:\n",
    "        return_value = \"X \"\n",
    "      elif field[1] == 1:\n",
    "        return_value = \"O \"\n",
    "      else:\n",
    "          return_value = str(c) + \" \"\n",
    "      if c < 6:\n",
    "          return_value += \"| \"\n",
    "      return return_value\n",
    "\n",
    "    def board_to_string(self, batch: Batch) -> str:\n",
    "        rows = batch.obs['obs']\n",
    "        row1 = self.field_to_string(rows[0][0][0], 0) + self.field_to_string(rows[0][1][0], 3) + self.field_to_string(rows[0][2][0], 6)\n",
    "        row2 = self.field_to_string(rows[0][0][1], 1) + self.field_to_string(rows[0][1][1], 4) + self.field_to_string(rows[0][2][1], 7)\n",
    "        row3 = self.field_to_string(rows[0][0][2], 2) + self.field_to_string(rows[0][1][2], 5) + self.field_to_string(rows[0][2][2], 8)\n",
    "        return row1 + '\\n' + row2 + '\\n' + row3\n",
    "\n",
    "    def ask_llm_for_choice(self, board: str, possible_choices) -> int:\n",
    "        job_description = \"You will be provided with a tic tac toe board. There are two players, X and O. An empty board looks likes this:\\n0 | 3 | 6\\n1 | 4 | 7\\n2 | 5 | 8\\nWhen a player made a move a X or O is placed on the board.\\nYou are player X and should choose the best possible option.\"\n",
    "        possible_choices_text = \", \".join(possible_choices)\n",
    "        question = \"The current board is: \\n\" + board + \"\\nThe possible numbers are \" + possible_choices_text + \". Only answer best number to choose, no comments or explanation, just a number.\"\n",
    "        output = self.generate(job_description, question, 0.1, 40)\n",
    "\n",
    "        extracted_choice = [int(i) for i in re.sub(r'[^0-9\\s]', '', output).split() if i.isdigit() and str(i) in possible_choices]\n",
    "        if extracted_choice:\n",
    "          print(\"Choice: \" + str(extracted_choice[0]))\n",
    "          return extracted_choice[0]\n",
    "        else:\n",
    "          print(\"No choice, return -1\")\n",
    "          return -1\n",
    "\n",
    "    def generate(self, job_description, question, temperature=0.7, max_new_tokens=512):\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": job_description,\n",
    "            },\n",
    "        ]\n",
    "        messages.append({\"role\": \"user\", \"content\": question})\n",
    "        prompt = self.pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        outputs = self.pipe(prompt, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature, top_k=50, top_p=0.95)\n",
    "        output = outputs[0][\"generated_text\"]\n",
    "        messages.append({\"role\": \"assistant\", \"content\": output})\n",
    "        response_start = output.rfind('<|assistant|>')\n",
    "        text_output = output[response_start + len('<|assistant|>'):]\n",
    "        return text_output\n",
    "\n",
    "    def forward(\n",
    "        self, batch: Batch, state: dict | Batch | np.ndarray | None = None\n",
    "    ) -> Batch:\n",
    "        board = self.board_to_string(batch)\n",
    "\n",
    "        if board in self.cache:\n",
    "          print(\"cache hit:\\n\" + board + \"\\nChoice: \" + str(self.cache[board]))\n",
    "          return Batch(act=[self.cache[board]])\n",
    "\n",
    "        all_choices = np.array(list(range(0,9)))\n",
    "        mask = batch.obs.mask.flatten()\n",
    "        masked_choices = all_choices[mask.astype(bool)].astype(str)\n",
    "\n",
    "        if len(masked_choices) == 1:\n",
    "          return Batch(act=[int(masked_choices[0])])\n",
    "\n",
    "        choice = -1\n",
    "        tries = 3\n",
    "        while str(choice) not in masked_choices and tries > 0:\n",
    "          choice = self.ask_llm_for_choice(board, masked_choices)\n",
    "          tries -= 1\n",
    "\n",
    "        if choice != -1 and str(choice) in masked_choices:\n",
    "          print(\"Add to cache:\\n\" + board + \"\\nChoice: \" + str(choice))\n",
    "          self.cache[board] = choice\n",
    "        else:\n",
    "          print(\"Invalid choice, pick first: \" + masked_choices[0])\n",
    "          return Batch(act=[int(masked_choices[0])])\n",
    "\n",
    "        return Batch(act=[choice])\n",
    "\n",
    "    def learn(self, batch: Batch) -> Dict[str, Any]:\n",
    "        return {}"
   ],
   "metadata": {
    "id": "cMT-wuoGRP2O"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Play with agents in all possible combinations\n",
    "\n",
    "1. Random - Random\n",
    "1. Random - DQN\n",
    "1. Random - LLM\n",
    "1. DQN - Random\n",
    "1. DQN - DQN\n",
    "1. DQN - LLM\n",
    "1. LLM - Random\n",
    "1. LLM - DQN\n",
    "1. LLM - LLM\n",
    "\n",
    "So every agents plays as player_1 against the other agents"
   ],
   "metadata": {
    "id": "bOgxaGu2sNS0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "llm_agent = LLMAgent()\n",
    "\n",
    "random_random = play(RandomPolicy(), RandomPolicy())\n",
    "random_dqn = play(RandomPolicy(), agent_learn_player1)\n",
    "random_llm = play(RandomPolicy(), llm_agent)\n",
    "dqn_random = play(agent_learn_player1, RandomPolicy())\n",
    "dqn_dqn = play(agent_learn_player1, agent_learn_player2)\n",
    "dqn_llm = play(agent_learn_player1, llm_agent)\n",
    "llm_random = play(llm_agent, RandomPolicy())\n",
    "llm_dqn = play(llm_agent, agent_learn_player2)\n",
    "llm_llm = play(llm_agent, llm_agent)"
   ],
   "metadata": {
    "id": "ba6gGX_usEWj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4463546b-4443-4e50-d094-b424b5dac1cf"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Final reward: 0.52, length: 7.66\n",
      "Win: 68 lost: 16 draw: 16\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Random - random: \" + str(random_random[0]/100))\n",
    "print(\"Random - DQN: \" + str(random_dqn[0]/100))\n",
    "print(\"Random - LLM: \" + str(random_llm[0]/100))\n",
    "print(\"DQN - random: \" + str(dqn_random[0]/100))\n",
    "print(\"DQN - DQN: \" + str(dqn_dqn[0]/100))\n",
    "print(\"DQN - LLM: \" + str(dqn_llm[0]/100))\n",
    "print(\"LLM - random: \" + str(llm_random[0]/100))\n",
    "print(\"LLM - DQN: \" + str(llm_dqn[0]/100))\n",
    "print(\"LLM - LLM: \" + str(llm_llm[0]/100))"
   ],
   "metadata": {
    "id": "5PoosEu-sc4G",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ef24bf5e-b637-4755-e55a-21c507d79e9f"
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random - random: 0.68\n",
      "Random - DQN: 0.28\n",
      "Random - LLM: 0.7\n",
      "DQN - random: 0.88\n",
      "DQN - DQN: 0.12\n",
      "DQN - LLM: 0.97\n",
      "LLM - random: 0.62\n",
      "LLM - DQN: 0.02\n",
      "LLM - LLM: 0.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ]
  }
 ]
}
