{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training PPO for BlackJack\n",
    "\n",
    "Used [PPOPolicy from Tianshou](https://tianshou.org/en/stable/03_api/policy/modelfree/ppo.html)."
   ],
   "metadata": {
    "id": "MWc4kcBMGnpg"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_ixzdtHHYg_a"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Install dependencies"
   ],
   "metadata": {
    "id": "y9lmQIv9F9Ke"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install gymnasium==0.29.1 pygame==2.3.0 pettingzoo==1.24.3 tianshou==0.5.1 transformers==4.39.1 accelerate==0.28.0 openai"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QSgELpG0YrrI",
    "outputId": "cff51a66-6129-473a-b42d-4299335a6f93"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup environment"
   ],
   "metadata": {
    "id": "GJUqMfTpGHkb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import tianshou as ts\n",
    "import torch\n",
    "from gymnasium.spaces import Dict\n",
    "from gymnasium.wrappers import FlattenObservation\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def get_env(render_mode = None):\n",
    "  \"\"\"\n",
    "  BlackJack has an observation space which is a tuple, consisting of\n",
    "  the player's sum, the dealers card showing and whether or not\n",
    "  the player has a usable ace. This tuple is flattened for Tianshou\n",
    "  to be able to put it through the deep network layers.\n",
    "  \"\"\"\n",
    "  env = gym.make(\"Blackjack-v1\", render_mode=render_mode)\n",
    "  env = FlattenObservation(env)\n",
    "  env.reset(seed=42)\n",
    "\n",
    "  return env\n",
    "\n",
    "env = get_env()"
   ],
   "metadata": {
    "id": "8eNTgrS7Qozg"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_envs = ts.env.DummyVectorEnv([get_env for _ in range(10)])\n",
    "test_envs = ts.env.DummyVectorEnv([get_env for _ in range(100)])"
   ],
   "metadata": {
    "id": "LP3tEh8Acfca"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Policy\n",
    "\n",
    "Create a deep DQN Policy using the network and optimizer."
   ],
   "metadata": {
    "id": "T4_YveVRRC87"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "state_shape = env.observation_space.shape or env.observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "\n",
    "net = ts.utils.net.common.Net(state_shape=state_shape, hidden_sizes=[128, 128, 128], device=device)\n",
    "\n",
    "actor = ts.utils.net.discrete.Actor(preprocess_net=net, action_shape=env.action_space.n, device=device).to(device)\n",
    "critic = ts.utils.net.discrete.Critic(preprocess_net=net, device=device).to(device)\n",
    "actor_critic = ts.utils.net.common.ActorCritic(actor, critic)\n",
    "optim = torch.optim.Adam(actor_critic.parameters(), lr=0.0003)\n",
    "\n",
    "policy = ts.policy.PPOPolicy(\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    optim=optim,\n",
    "    dist_fn=torch.distributions.Categorical,\n",
    "    action_space=env.action_space,\n",
    "    action_scaling=False,\n",
    ")"
   ],
   "metadata": {
    "id": "KJ-3vuAuREa8"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_collector = ts.data.Collector(\n",
    "    policy, train_envs, ts.data.VectorReplayBuffer(20000, 10), exploration_noise=True\n",
    ")\n",
    "test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)"
   ],
   "metadata": {
    "id": "5YDAB_hZRGR8"
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the agent\n",
    "Using the OffPolicyTrainer."
   ],
   "metadata": {
    "id": "wgZy7_UPJzbJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "result = ts.trainer.OnpolicyTrainer(\n",
    "    policy=policy,\n",
    "    batch_size=64,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=10,\n",
    "    step_per_epoch=10000,\n",
    "    repeat_per_collect=10,\n",
    "    episode_per_test=100,\n",
    "    step_per_collect=10\n",
    ").run()\n",
    "print(f'Finished training! Use {result[\"duration\"]}')\n",
    "\n",
    "result"
   ],
   "metadata": {
    "id": "Sb9XhtchcB3G",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8355d7bd-d60f-41d2-eb51-1830cf1b60aa"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Play\n",
    "\n",
    "Play with the trained agent to the opponent a number of episodes and print the results"
   ],
   "metadata": {
    "id": "nxLtBMTBKNg-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "policy.eval()\n",
    "\n",
    "env = get_env(render_mode=None)\n",
    "env = ts.env.DummyVectorEnv([lambda: env])\n",
    "collector = ts.data.Collector(policy, env, exploration_noise=True)\n",
    "# play number of episodes\n",
    "result = collector.collect(n_episode=100, render=None)\n",
    "rews, lens = result[\"rews\"], result[\"lens\"]\n",
    "\n",
    "won = 0\n",
    "draw = 0\n",
    "lost = 0\n",
    "for res in result['rews']:\n",
    "  if res == 1:\n",
    "    won += 1\n",
    "  elif res == -1:\n",
    "    lost +=1\n",
    "  else:\n",
    "    draw += 1\n",
    "\n",
    "print(\"Win: \" + str(won) + \" lost: \" + str(lost) + \" draw: \" + str(draw))"
   ],
   "metadata": {
    "id": "MAhLVoCMctbb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "aee84ae0-6f8a-44ed-c298-3326f5f27dcd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "wX5mHWKuUvjf"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
