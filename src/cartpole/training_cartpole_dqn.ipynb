{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWc4kcBMGnpg"
      },
      "source": [
        "# Training DNQ for CartPole\n",
        "\n",
        "Based on [Deep Q-Network from Tianshou](https://tianshou.org/en/stable/01_tutorials/00_dqn.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ixzdtHHYg_a"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9lmQIv9F9Ke"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSgELpG0YrrI"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium==0.29.1 pygame==2.3.0 pettingzoo==1.24.3 tianshou==0.5.1 transformers==4.39.1 accelerate==0.28.0 plotly openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJUqMfTpGHkb"
      },
      "source": [
        "# Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eNTgrS7Qozg"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import tianshou as ts\n",
        "\n",
        "def get_env(render_mode=None):\n",
        "  return gym.make(\"CartPole-v1\", render_mode=render_mode)\n",
        "\n",
        "env = get_env()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP3tEh8Acfca"
      },
      "outputs": [],
      "source": [
        "train_envs = ts.env.DummyVectorEnv([get_env for _ in range(10)])\n",
        "test_envs = ts.env.DummyVectorEnv([get_env for _ in range(100)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB6dVGoGGPl0"
      },
      "source": [
        "# Setup PyTorch Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSDEGYrcZUO2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, state_shape, action_shape):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(np.prod(state_shape), 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, np.prod(action_shape)),\n",
        "        )\n",
        "\n",
        "    def forward(self, obs, state=None, info={}):\n",
        "        if not isinstance(obs, torch.Tensor):\n",
        "            obs = torch.tensor(obs, dtype=torch.float)\n",
        "        batch = obs.shape[0]\n",
        "        logits = self.model(obs.view(batch, -1))\n",
        "        return logits, state\n",
        "\n",
        "\n",
        "state_shape = env.observation_space.shape or env.observation_space.n\n",
        "action_shape = env.action_space.shape or env.action_space.n\n",
        "\n",
        "net = Net(state_shape, action_shape)\n",
        "optim = torch.optim.Adam(net.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4_YveVRRC87"
      },
      "source": [
        "# Create Policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ-3vuAuREa8"
      },
      "outputs": [],
      "source": [
        "policy = ts.policy.DQNPolicy(\n",
        "    model=net,\n",
        "    optim=optim,\n",
        "    action_space=env.action_space,\n",
        "    discount_factor=0.9,\n",
        "    estimation_step=3,\n",
        "    target_update_freq=320,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YDAB_hZRGR8"
      },
      "outputs": [],
      "source": [
        "train_collector = ts.data.Collector(\n",
        "    policy, train_envs, ts.data.VectorReplayBuffer(20000, 10), exploration_noise=True\n",
        ")\n",
        "test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgZy7_UPJzbJ"
      },
      "source": [
        "# Train the agent\n",
        "Using the OffPolicyTrainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sb9XhtchcB3G"
      },
      "outputs": [],
      "source": [
        "result = ts.trainer.OffpolicyTrainer(\n",
        "    policy=policy,\n",
        "    train_collector=train_collector,\n",
        "    test_collector=test_collector,\n",
        "    max_epoch=10,\n",
        "    step_per_epoch=10000,\n",
        "    step_per_collect=10,\n",
        "    update_per_step=0.1,\n",
        "    episode_per_test=100,\n",
        "    batch_size=64,\n",
        "    train_fn=lambda epoch, env_step: policy.set_eps(0.1),\n",
        "    test_fn=lambda epoch, env_step: policy.set_eps(0.05),\n",
        "    stop_fn=lambda mean_rewards: mean_rewards >= env.spec.reward_threshold,\n",
        ").run()\n",
        "print(f'Finished training! Use {result[\"duration\"]}')\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxLtBMTBKNg-"
      },
      "source": [
        "# Play\n",
        "\n",
        "Play with the trained agent to the opponent a number of episodes and print the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAhLVoCMctbb"
      },
      "outputs": [],
      "source": [
        "policy.eval()\n",
        "\n",
        "env = get_env(render_mode=None)\n",
        "env = ts.env.DummyVectorEnv([lambda: env])\n",
        "collector = ts.data.Collector(policy, env, exploration_noise=True)\n",
        "result = collector.collect(n_episode=100, render=None)\n",
        "rews, lens = result[\"rews\"], result[\"lens\"]\n",
        "\n",
        "display(rews.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPwVK7sUZ7H7"
      },
      "source": [
        "Plot Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77RNG1EUUA-P"
      },
      "outputs": [],
      "source": [
        "import plotly.figure_factory as ff\n",
        "\n",
        "fig = ff.create_distplot([result['rews']], ['reward'])\n",
        "fig.update_layout(title_text='CartPole DQN Result')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wX5mHWKuUvjf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
